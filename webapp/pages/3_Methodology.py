import sys
import os
import streamlit as st

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.insert(0, project_root)

from src.utils.stats import (
    basic_counts, top_k_words, word_length, 
    split_sentences, sentence_statistics
)

st.title("Methodology")
st.write("Detailed explanation of implementation approaches and techniques used in this project.")

st.divider()

# Dataset Details Section
st.header("Dataset Details")
st.write("""
This project uses a dataset of 12,000 restaurant reviews with ratings from 1 to 5 stars. 
The dataset was balanced to create three sentiment classes:

- **Negative**: Ratings 1-2 (4,000 reviews each)
- **Neutral**: Rating 3 (4,000 reviews)
- **Positive**: Ratings 4-5 (2,000 reviews each)

The reviews contain real-world text with varying lengths, informal language, and natural sentiment expressions,
making it a practical testbed for comparing different NLP approaches.
""")

st.divider()

# Rule-Based Section
st.header("1. Rule-Based Technique")

st.subheader("Overview")
st.write("""
Rule-based NLP is not ML-based. It relies solely on linguistic rules and patterns to try to understand human language. 
But these "rules" have limitations and therefore struggle to pick up on the real meaning and semantics of the text. 
It is purely a **syntax-based technique** and, out of the three techniques, obviously tends to perform the weakest, 
as the other methods are more complex and try to understand the true meaning rather than just focus on the syntax.
""")

st.subheader("Tokenization")
st.write("""
I used rule-based technique for tokenization, sentiment analysis, as well as topic detection. For tokenization, 
I used something called **regular expression (regex)**, which performs a search pattern to match and manipulate text. 
I utilized the `re` library of Python for this. With the help of regex, I tokenized texts and removed **stop words** 
such as (I, the, a, me, etc). Stop words are words that frequently pop up and therefore don't really add much value 
to the text. Not to mention, they especially don't help you with finding the sentiment of the text.
""")

st.subheader("Sentiment Analysis Logic")
st.write("""
For sentiment analysis using rule-based, I continued to use the tokens generated by my `regex_tokenizer`. 
The logic for this was quite simple:

1. I have three sets: `POSITIVE_WORDS`, `NEGATIVE_WORDS`, and `NEGATION_WORDS`
2. Every time a token appears in my positive words set, I increment its value by one
3. Same logic for the tokens that appeared in the negative set
4. I also had to consider **negations** in human language

**Example of negation handling:**  
Without negation, the following sentence would be ruled out as positive: *"The pizza was not good!"*  
Clearly this is a negative sentiment, but since the whole input sequence contains one positive word "good", 
this would be ruled out as positive. So negation matters because if you have a word like "never" before "good" 
(*"never good"*), then this should actually be counted as negative, not positive.
""")

st.subheader("Classification & Confidence")
st.write("""
After looping through the tokens, we get the final sum for the total number of positive and negative words:

- If `num_positive > num_negative` ‚Üí classify as **positive**
- If `num_negative > num_positive` ‚Üí classify as **negative**  
- If `num_positive == num_negative` ‚Üí classify as **neutral**

Along with the prediction, I also return the **confidence** of each prediction using the normalized confidence formula:

`confidence = |pos - neg| / (pos + neg)`

This returns a range from 0 to 1.
""")

st.subheader("Performance")
st.write("""
This technique achieved an **accuracy of ~41%** on my dataset. While VERY quick, the performance really wasn't that good. 
At first, it looks like it performed "better" than the LLM & BERT method for classifying the `neutral` class. 
But really it just predicted `neutral` a lot for most data points, and picking `neutral` shows that rule-based has 
ambiguity and didn't commit to either `positive` or `negative` class.

My positive & negative words lists weren't really that long, so perhaps a longer set would improve performance. 
But that's a maybe, and rule-based has limits with its use cases‚Äîsentiment analysis isn't one that it excels at.
""")

st.subheader("Additional Stats Implementation")
st.write("""
I also tried doing some rule-based stats in my `stats.py` for fun & from scratch, including:

- **Basic counts**: Number of characters, words, and sentences
- **Top K words**: Most frequent words in the text
- **Word length statistics**: Longest word, shortest word, average word length
- **Sentence splitting**: Breaking input into individual sentences
- **Sentence statistics**: Longest sentence, shortest sentence, average sentence length
""")

# Interactive Stats Demo
st.subheader("Try the Stats Functions!")
st.write("Enter text below to see rule-based text statistics in action:")
st.write("""Example text: I am so impressed with the Vanicream Daily Facial Moisturizer. My skin is very sensitive, and this is one of the only products that keeps it calm, hydrated, and irritation-free. It has a smooth, lightweight texture that absorbs quickly without leaving my face greasy or sticky.
I also love that it‚Äôs fragrance-free and packed with hyaluronic acid and ceramides which makes my skin feels soft, balanced, and moisturized all day long. After using it consistently, I can definitely see a difference in how healthy and even my skin looks.
If you have sensitive or acne prone skin, I highly recommend this. I‚Äôm genuinely satisfied with my results and will absolutely repurchase!""")

user_stats_input = st.text_area(
    "Enter your text for analysis",
    placeholder="Type or paste any text here to analyze its statistics...",
    height=120
)

if st.button("Analyze Text Statistics"):
    if user_stats_input:
        st.write("---")
        
        # Basic Counts
        num_chars, num_words, num_sentences = basic_counts(user_stats_input)
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Characters", num_chars)
        with col2:
            st.metric("Words", num_words)
        with col3:
            st.metric("Sentences", num_sentences)
        
        st.write("")
        
        # Word Statistics
        st.write("**Word Statistics:**")
        longest, shortest, avg_len = word_length(user_stats_input)
        col1, col2, col3 = st.columns(3)
        with col1:
            st.write(f"Longest word: **{longest}** ({len(longest)} chars)")
        with col2:
            st.write(f"Shortest word: **{shortest}** ({len(shortest)} chars)")
        with col3:
            st.write(f"Avg word length: **{avg_len}** chars")
        
        st.write("")
        
        # Top K Words
        st.write("**Top 5 Most Frequent Words:**")
        top_words = top_k_words(user_stats_input, 5)
        st.write(", ".join([f"**{word}**" for word in top_words]))
        
        st.write("")
        
        # Sentence Statistics
        if num_sentences > 0:
            st.write("**Sentence Statistics:**")
            try:
                longest_sen, shortest_sen, avg_sen = sentence_statistics(user_stats_input)
                st.write(f"- Longest sentence: *\"{longest_sen}\"* ({len(longest_sen.split())} words)")
                st.write(f"- Shortest sentence: *\"{shortest_sen}\"* ({len(shortest_sen.split())} words)")
                st.write(f"- Average sentence length: **{avg_sen}** words")
            except:
                st.write("Could not analyze sentence statistics for this text.")
    else:
        st.warning("Please enter some text to analyze!")

st.divider()

# BERT Section
st.header("2. Bidirectional Encoder Representations from Transformers (BERT)")

st.subheader("Overview")
st.write("""
BERT was a popular encoder-only model released by Google in 2018. It was a revolutionary language model that 
could process text **bidirectionally**‚Äîmeaning it could understand context from both directions simultaneously. 
Unlike the original Transformers architecture, BERT doesn't have a decoder component and therefore cannot generate text. 
Hence, why it's called an **encoder-only model**, as the architecture of BERT is essentially a bunch of encoders stacked up.

There are also two BERT model sizes:
- **BERT base**: 12 transformer encoder layers
- **BERT large**: 24 transformer encoder layers

For this project, I utilized **BERT base** as my dataset is fairly small.
""")

st.subheader("BERT's Task #1: Masked Language Modeling (MLM)")
st.write("""
BERT typically produces two main types of outputs: `last_hidden_state` (sequence out) and `pooler_output` 
(pooled representation for the whole sequence).

The `last_hidden_state` gets us the last encoder's values. Each token is a **768-dimensional vector**, and each token 
knows about all the other tokens in the input. This last output is the deep semantic representation of your input sequence. 
This is also BERT's task #1, which is **Masked Language Modeling (MLM)** ‚Äî you randomly mask words and predict them.
""")

st.subheader("BERT's Task #2: Next Sentence Prediction (NSP)")
st.write("""
The other main output of BERT was `pooler_output`, which takes the **[CLS] token embedding** from the `last_hidden_state` 
and passes it through a linear layer and tanh activation. It represents the whole input sequence, designed specifically 
for classification tasks‚Äîexactly the use case for sentiment analysis classification.

This exists because BERT uses [CLS] for **Next Sentence Prediction (NSP)**, which was the second task of BERT: 
does sentence B actually follow sentence A in the original text?
""")

st.subheader("Sentiment Classification Using BERT")
st.write("""
The task is to train BERT to read reviews and make it classify the sentiment: **positive, negative, or neutral**.

We first start off by selecting a sequence length. We analyze the token length distribution for most of our reviews. 
For BERT, the maximum length is 512 tokens, but we don't need to use all 512 as our sequence length. Most reviews 
were length 0-100, therefore, I set the sequence length at **128 tokens**.

**Why shorter sequence length?**
- ‚úì Improves computational efficiency by reducing GPU memory usage
- ‚úì Faster training time
- ‚úì Ideal for sentiment analysis where context beyond a few hundred tokens may not be too relevant
""")

# Try to display the sequence length chart
try:
    st.image(os.path.join(project_root, 'webapp', 'assets', 'seq_length.png'), 
             caption="Distribution of Review Token Lengths", 
             use_container_width=True)
except Exception as e:
    st.info(f"Sequence length distribution chart (webapp/assets/seq_length.png)")

st.subheader("Prepare PyTorch Dataset")
st.write("""
For tiny datasets (<100 samples), you could tokenize everything and batch up front, but since we have thousands 
of samples, we first need to create a **PyTorch dataset** from our original dataset. This helps us with:
- Batching
- Shuffling
- Preprocessing
- Memory efficiency
- Multi-processing

It's clean, scalable, efficient code and the best way to handle data in PyTorch.

In my `PTDataset(data.Dataset)` function, we pass in:
- **text**: List of reviews
- **target**: List of labels (0, 1, 2 for negative, neutral, positive)
- **tokenizer**: BERT's tokenizer
- **max_len**: Max tokens per review (128 in our case)
""")

st.write("""
The `encoding` is the tokenizer's output. It converts the text/review into numbers for BERT to understand/process. 
What `encode_plus` does is:

1. **Tokenizes** the words in our review
2. **Converts them into IDs**
3. **Adds special tokens** ([CLS] at start, [SEP] at the end)
4. **Pads to max_length** to fill remaining spots with [PAD] tokens
5. **Creates attention_mask** which marks which tokens to focus on: focused gets (1), ignore gets (0)

The two main final returns to focus on are **input_ids** and **attention_mask**, as those are the two main 
parameters for our output with BERT.
""")

with st.expander("üìù Example: Text Transformation Process"):
    st.code("""
Review: "The chicken was amazing!"

1) Tokenizes: ['the', 'chicken', 'was', 'amazing', '!']

2) Convert to IDs: 
   'the' -> 6249, 'chicken' -> 4856, 'was' -> 3455, 
   'amazing' -> 2332, '!' -> 5437

3) Add special tokens: 
   [CLS] = 101, [SEP] = 102

4) Pads to max_length (128): 
   [101, 6249, 4856, 3455, 2332, 5437, 102, 0, 0, 0, ‚Ä¶]
   (the 0's continue up to 128 tokens)

5) Create attention_mask: 
   [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ‚Ä¶]
   (marks 1 for the first 7 real tokens)
    """, language="python")

st.subheader("Batch Size & Epochs")
st.write("""
Next, we select the `batch_size` and the number of `epochs`:

- **Batch size**: 8
  - From our 12K dataset: 12,000 / 8 = 1,500 batches per epoch
  - Model updates weights 1,500 times per epoch

- **Epochs**: 20 (initial setting)
  - 20 √ó 1,500 = 30,000 total weight updates during training

**Actual split:**
- Training: 9,600 samples (80%)
- Validation: 1,200 samples (10%)
- Testing: 1,200 samples (10%)

So really: 9,600 / 8 = 1,200 batches per epoch ‚Üí 20 √ó 1,200 = **24,000 total weight updates**
""")

st.subheader("Building the Classifier")
st.write("""
I define my model architecture in my `SentimentClassifier(nn.Module)` class, which takes:
- **n_classes**: Number of sentiment categories (3)

**What the model contains (layers):**
- **bert**: Our uncased base BERT model
- **drop**: Dropout rate set to 0.30
  - This is the probability of randomly deactivating neurons during training
  - Prevents overfitting by not depending on specific neurons
  - Forces model to learn more robust and redundant features that generalize better
- **out**: Linear layer that maps 768 BERT dimensions into 3 classes
- **softmax**: Converts scores to probabilities (not used in training but for prediction)

**The forward() function:**
- Takes in `input_ids` and `attention_mask`
- After BERT processes our input, it returns the `pooler_output` mentioned earlier
- Applies dropout, linear layer, and returns raw scores (logits) for each class
- Output is a tensor of shape [batch_size, 3] with scores for [negative, neutral, positive]

**Example output:** `[[0.2, -0.5, 2.1]]` ‚Üí predicts **positive** (highest score)
""")

st.subheader("Training & Evaluation")
st.write("""
For training evaluation, I used two primary metrics: **accuracy** and **macro-F1**.

- **Accuracy**: Simply how many samples the model got correct
  - Problem: Treats all mistakes as the same and doesn't care which class is wrong

- **Macro F1**: Does a better job showing how well the model does on each class
  - Example: May perform really well at picking up `positive` class but struggle with `neutral`

**Overfitting Problem:**  
Originally, I was training to all 20 epochs (~30 minutes on an A100 GPU). While training accuracy was improving, 
the validation score was plateauing. This is a sign of **overfitting** as the model was memorizing the training 
set but not generalizing well with new data.

**Solution: Early Stopping**  
I applied early stopping where training stops once the macro F1 score stops improving. Once implemented, 
our model stopped training at around **4-5 epochs**. This makes sense as a large model like BERT with millions 
of parameters usually learns most relevant information on a small dataset (~12K) by 2-5 epochs.

**Final Results:**
- Stopped at epoch 5
- Training accuracy: 93%
- Validation accuracy: 72%
- Macro F1 score: 0.71
- Training time: ~5 minutes
""")

st.subheader("Testing Results")
st.write("""
When training stopped at epoch 5, we used that as our best model and loaded it to test on our test split. 
Accuracy for the test set was about **71%**, which is reasonable considering reviews can be inherently noisy.

**Example of ambiguous review:**  
*"Food was ok, drinks were really good, but prices are fair"*  
Even for humans, it can be difficult to confidently classify this as positive or neutral.

This is reflected in our precision, recall, and F1 report, where our model struggles most with classifying 
the **neutral class**. However, on clearer examples, BERT demonstrates strong semantic understanding of text. 
You can test it on the home page!
""")

# Try to display the classification report
try:
    st.image(os.path.join(project_root, 'webapp', 'assets', 'class_report.png'), 
             caption="BERT Classification Report", 
             use_container_width=True)
except Exception as e:
    st.info(f"Classification report image (webapp/assets/class_report.png)")

st.divider()

# LLM Section
st.header("3. LLM Approach (Gemini-2.5-Flash)")

st.subheader("Overview")
st.write("""
For the LLM approach, I did **not fine-tune** the model and the code for this is fairly simple. I utilized 
Google's **gemini-2.5-flash** model, since it's free! But unfortunately it has a rate limit of 20 API calls 
per day. So I used the **30-day free trial** for Google's AI Studio to test the model.

To use Gemini, you first need to:
1. Install `google-genai`
2. Create an API key

After that, you are pretty much set to start using Gemini!
""")

st.subheader("Customization Options")
st.write("""
There are a number of ways to customize Gemini. For example, you can activate **"Thinking"** mode which lets 
your model reason, and you can even look line by line at its reasoning process.

More info about Gemini's API documentation and how to get started:  
[https://ai.google.dev/gemini-api/docs/quickstart](https://ai.google.dev/gemini-api/docs/quickstart)
""")

st.subheader("Testing Approach")
st.write("""
For the LLM approach, I didn't utilize all 12K data samples‚ÄîI would've quickly hit the rate limit right away. 
So I first decided to test with only **1% of my dataset with 120 samples**.

While I knew that LLMs via API are slower than BERT, I didn't know it was **THAT much slower**.
""")

st.subheader("Speed Comparison: BERT vs LLM")

col1, col2 = st.columns(2)
with col1:
    st.metric("BERT Training", "~30 minutes", "9,600 samples")
    st.write("**Per sample:** ~0.19s")
with col2:
    st.metric("Gemini Inference", "~5 min 34s", "120 samples")
    st.write("**Per sample:** ~2.8s")

st.write("""
**Extrapolation for full dataset:**  
If Gemini processed 9,600 samples at ~2.8s per sample:
- Total time: ~33,400 seconds
- **~556.67 minutes** or **~9.27 hours**

This is roughly **18x slower** than BERT training!
""")

st.subheader("Why the Speed Difference?")
st.write("""
At first, I thought it was due to **LLM API latency vs BERT local** issue. But it's more about the model's 
architecture and size rather than local vs API:

**Modern LLMs:**
- Tend to have **billions of parameters**, which means more compute per forward pass
- Process entire conversation history with each new token
- Attention mechanism scales **quadratically** with sequence length

**BERT Advantages:**
- Massive advantage with just **one forward pass** (entire input in one go)
- Outputs all predictions simultaneously
- Has much **fewer parameters** meaning faster computation
- After initial training, inference is extremely fast
""")

st.subheader("Prompt Engineering")
st.write("""
For the LLM approach, I used prompt engineering to guide the model's sentiment analysis. The prompt instructs 
the model to:

1. Classify sentiment as positive, negative, or neutral
2. Provide a confidence score (0-100)
3. Include reasoning for the classification

This zero-shot approach (no training examples) achieved **~74% accuracy** on the test set, slightly outperforming 
BERT's 71% without any model training.
""")

with st.expander("üìù View Example Prompt"):
    st.code("""
Analyze the sentiment of the given text and provide:
1. Classification: positive, negative, or neutral
2. Confidence score: 0-100 representing your certainty
3. Reasoning: Brief explanation for your classification

Return JSON format:
{
    "sentiment": "positive|negative|neutral",
    "confidence": 85,
    "reasoning": "Contains strong positive words..."
}
    """, language="text")

st.subheader("Final Verdict")
st.write("""
Overall, for this specific task, **BERT is the ideal choice**. Better prompting and better models could get you 
better results with LLM, or when you don't have training data. But BERT has an amazing balance of:

‚úì **Accuracy**: 71% (very competitive)  
‚úì **Speed**: A LOT faster (18x faster than LLM)  
‚úì **Cost**: Essentially free after initial training  
‚úì **Predictability**: Consistent latency and performance

**When to use LLM:**
- You don't have training data
- You need explanations/reasoning for predictions
- Accuracy is more important than speed
- You can afford API costs

**When to use BERT:**
- You have training data available
- Speed and efficiency matter
- You want predictable costs
- Production deployment with high volume

*Note: You could train BERT on Google Colab with free GPU, but training time will probably be in the hours 
or days, depending on the size of your dataset. Having access to an A100 GPU significantly speeds up the process!*
""")
